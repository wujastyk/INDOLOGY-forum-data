[INDOLOGY] Google Translate for Sanskrit
 <CADKD_zh4etGYUtcSYZ+zLO0X-uAcd3xn9NbyChP76_WLakKNDA@mail.gmail.com>
 <E0B1DFD3-71CE-4D82-BF21-DFA959B0CB16@gmail.com>
 <CAKdt-CfBvgLYaFORPA=Lzn=dHqhuz2TLJTLpmEfvDeNqT3=dpg@mail.gmail.com>
 <CAB3-dzeLX57veazsNE4BkL3tQ0uF0o0tzUVgmKFPrc80VUD=yA@mail.gmail.com>
 <CAB3-dzdv8+z+fnZ0qSaCXtX=SrNepMT_Ef7Kfk_A2obyfWux2g@mail.gmail.com>
 <SA2PR08MB6569E66435DC6A50E18249C8E7CB9@SA2PR08MB6569.namprd08.prod.outlook.com>
 <8F91189C-CA5E-4B0F-A1C7-1EE2FFFC1C24@gmail.com>
 <20220513100100.56ef14bb3fe26fe91d445084@univ-reims.fr>
 <CAEjEOsDWML7YtjanrOjQFESxUrcNywMZUOtxNAKANs+utxB3mg@mail.gmail.com>
 <d2335d10-736b-046b-6f38-f6acd3eca402@gmx.de>
 <20220513130425.fd7ca4cd2491b8645f4db24f@univ-reims.fr>
Looking at the characteristics on these translations, their rather
good performance on what could be considered 'modern Sanskrit' and the
not-so-convincing performance on older material, I have my doubts
whether this is really a zery shot approach; I rather assume that they
used a (zero-shot?) language model to bootstrap a bitext mining system
that then generated Sanskrit<>English translation pairs based on the
Sanskrit WIkipedia and other related resources. With a small amount of
such data a T5-like seq2seq architecture that was pretrained on a lot
of (English) data could then be used to output nicely formed (but not
always correct) translations.
Even without much specific vocabulary, this system doesn't seem to be
very robust I am afraid.
For example a famous verse from Maitreya's Madhy?ntavibh?ga:
na ??nya? n?pi c???nya? tasm?t sarva? vidh?yate |
sattv?dasattv?t sattv?cca madhyam? pratipacca s? || 1.3
Google translate:
"Neither empty nor empty, therefore everything is prescribed
That is the middle counterpart from Sattva to Sattva"
There is no very specific vocabulary here but the translation is still
quite meh.
Now when we run the Chinese translation (??, 7th century) of this
verse through a machine translation model that we are currently
developing for Buddhist Chinese, we get a very different result:
????????????
????????????
therefore it is said that all dharmas are neither empty nor nonempty
because of both existence and nonexistence, this is the middle way
For Chinese, we also don't have much parallel data for the training of
the models, but we can take advantage of the large bilingual corpora
for non-Classical/non-Buddhist material.
The different results of these two models show rather clearly to me
that zero shot transfer learning for low resource languages such as
Sanskrit sounds nice in theory but is extremely difficult to do well
on target-domains in practice. For Chinese, on the other hand, the
sheer mass of bilingual training data that we have enables transfer
learning from a non-domain-specific corpus into a very narrow domain
and produce surprisingly good results. Who knows what our models can
do in the near future, but I guess it will take a while until we get
universally good Sanskrit machine translations...
Best,
Sebastian Nehrdich
On Fri, May 13, 2022 at 1:05 PM Satyanad Kichenassamy
<satyanad.kichenassamy at univ-reims.fr> wrote:
