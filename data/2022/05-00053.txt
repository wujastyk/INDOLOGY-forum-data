[INDOLOGY] Google Translate for Sanskrit
 <CADKD_zh4etGYUtcSYZ+zLO0X-uAcd3xn9NbyChP76_WLakKNDA@mail.gmail.com>
 <E0B1DFD3-71CE-4D82-BF21-DFA959B0CB16@gmail.com>
 <CAKdt-CfBvgLYaFORPA=Lzn=dHqhuz2TLJTLpmEfvDeNqT3=dpg@mail.gmail.com>
 <CAB3-dzeLX57veazsNE4BkL3tQ0uF0o0tzUVgmKFPrc80VUD=yA@mail.gmail.com>
 <CAB3-dzdv8+z+fnZ0qSaCXtX=SrNepMT_Ef7Kfk_A2obyfWux2g@mail.gmail.com>
 <SA2PR08MB6569E66435DC6A50E18249C8E7CB9@SA2PR08MB6569.namprd08.prod.outlook.com>
 <8F91189C-CA5E-4B0F-A1C7-1EE2FFFC1C24@gmail.com>
 <20220513100100.56ef14bb3fe26fe91d445084@univ-reims.fr>
 <CAEjEOsDWML7YtjanrOjQFESxUrcNywMZUOtxNAKANs+utxB3mg@mail.gmail.com>
Most probably they have built their MT system on top of so called deep
contextualized embeddings such as BERT
(https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)
or Roberta (https://huggingface.co/docs/transformers/model_doc/roberta).
We have analyzed such multilingual embeddings for a Sanskrit project,
and it turned out that the Sanskrit data were mainly taken from a dump
of the Sanskrit Wikipedia, which explains the preference for the modern
version. Very useful for MT, less so for a close reading of Vedic texts.
Best, Oliver
On 13/05/2022 11:51, Antonia Ruppel via INDOLOGY wrote:
